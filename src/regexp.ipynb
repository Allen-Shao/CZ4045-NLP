{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_with_code = pd.read_csv(\"../processed_data/remove_duplicates_selected_codes.csv\")\n",
    "post_with_code.set_index('PostId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_code_blks = pd.read_csv(\"../processed_data/code_top300.csv\")\n",
    "selected_code_blks.set_index('PostId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "identifier = r'[a-zA-Z_][\\d\\w_]*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = r'''break|default|func|interface|select|case|defer\n",
    "                |go|map|struct|chan|else|goto|package|switch\n",
    "                |const|fallthrough|if|range|type|continue|for|import|return|var'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# operators and punctuation\n",
    "operators = r'[\\+\\-\\*\\,;\\$><!:\\.\\|&\\^=\\(\\)\\[\\]\\{\\}]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decimal_literal = r'\\d+i?'           \n",
    "octal_literal = r'0[1-7]*'\n",
    "hex_literal = r'0[xX][a-fA-F]+'\n",
    "floating_literal = r''' \\d+\\.\\d*(?:[eE][+-]\\d+)?i?     \n",
    "                | \\d+[eE][+-]\\d+i?               \n",
    "                | \\.\\d+(?:[eE][+-]\\d+)?i?        \n",
    "                '''\n",
    "string_literal = r'''\\\"\\s*.*\\n?\\\"'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = r'//.*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patterns =  keywords + '|' + identifier + '|' + operators + '|' \\\n",
    "        + decimal_literal + '|' + octal_literal + '|' + hex_literal + '|' \\\n",
    "        + floating_literal + '|' + string_literal + '|' + comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(patterns)\n",
    "natural_lang_tokenizer = RegexpTokenizer(r'\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = pd.read_csv('../processed_data/answers.csv')\n",
    "qns = pd.read_csv('../processed_data/questions.csv')\n",
    "answers.set_index('PostId', inplace=True)\n",
    "qns.set_index('PostId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def annotatet_code(code, dic, post):\n",
    "    code_lines = list(filter(None, code.split('\\n')))\n",
    "    for line in code_lines:\n",
    "        # store line info then append to 'post' array\n",
    "        line_info = [line]\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        line_info.append(tokens)\n",
    "        dic['tokens'] += (tokens)\n",
    "        # start annotation\n",
    "        ann = []\n",
    "        for t in tokens:\n",
    "            if re.match(comments, t):\n",
    "                ann.append('COMMENT')\n",
    "                t = t.replace('//', '')\n",
    "                # annotate comments as natural language\n",
    "                t = natural_lang_tokenizer.tokenize(t)\n",
    "                line_info.pop()\n",
    "                line_info.append(['//']+t)\n",
    "                dic['tokens'].pop()\n",
    "                dic['tokens'] += t\n",
    "                tags = nltk.pos_tag(t)\n",
    "                ann += [tags[i][1] for i in range(len(tags))]\n",
    "            elif re.match(keywords, t):\n",
    "                ann.append('KEYWORD')\n",
    "            elif re.match(identifier, t):\n",
    "                ann.append('IDENTIFIER')\n",
    "            elif re.match(operators, t):\n",
    "                ann.append('OPERATOR')\n",
    "            elif re.match(decimal_literal, t):\n",
    "                ann.append('DECIMAL_LITERAL')\n",
    "            elif re.match(octal_literal, t):\n",
    "                ann.append('OCTAL_LITERAL')\n",
    "            elif re.match(hex_literal, t):\n",
    "                ann.append('HEX_LITERAL')\n",
    "            elif re.match(string_literal, t):\n",
    "                ann.append('STRING_LITERAL')\n",
    "            elif re.match(floating_literal, t):\n",
    "                ann.append('FLOATING_LITERAL')\n",
    "            else:\n",
    "                ann.append('UNDEFINED')\n",
    "        line_info.append(ann)\n",
    "        dic['anns'] += (ann)\n",
    "        post.append(line_info)\n",
    "    return dic, post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations is used to store all the 100 posts\n",
    "# each element in this array should be one particular post\n",
    "annotations = []\n",
    "post_number = 0\n",
    "# iterate through 150 posts in case there is any 'no match' problem\n",
    "# Note: the 'no-mathc' problem is due to different formatting/parsing methods from xml to csv\n",
    "for p_id in post_with_code.index[:150]:\n",
    "    no_match = False\n",
    "    if p_id in answers.index:\n",
    "        body = answers.loc[p_id]['Body']\n",
    "    elif p_id in qns.index:\n",
    "        body = qns.loc[p_id]['Body']\n",
    "    # separate code and natural language, store into parts\n",
    "    parts = []\n",
    "    # 1 for code, 0 for natural language\n",
    "    code_or_not = []\n",
    "    all_code_blks = selected_code_blks.loc[[p_id], 'Code']\n",
    "    position = {}\n",
    "    \n",
    "    for code_blk in all_code_blks.values:\n",
    "        if body.find(code_blk) == -1:\n",
    "            print(str(p_id) + \" code not match body\")\n",
    "            no_match = True\n",
    "            break\n",
    "        if len(code_blk) > 20:\n",
    "            position[body.index(code_blk)] = code_blk \n",
    "    if no_match:\n",
    "        continue\n",
    "    keylist = sorted(position.keys())\n",
    "    \n",
    "    for key in keylist:\n",
    "        code = position[key]\n",
    "        rest = body.split(code)\n",
    "        # handle the situation where the code appears more than once -- split the rest into more than 2 parts\n",
    "        for splited in rest[:-1]:\n",
    "            parts += ([splited, code])\n",
    "            code_or_not += ([0, 1] if splited else [1])\n",
    "        if len(rest) > 2:\n",
    "            parts.append(rest[-1])\n",
    "            if rest[-1].strip():\n",
    "                code_or_not.append(0)\n",
    "                \n",
    "        parts = list(filter(None, parts))\n",
    "        body = rest[-1]\n",
    "    if len(rest) == 2 and rest[1].strip():\n",
    "        parts.append(rest[1]) \n",
    "        code_or_not.append(0)\n",
    "    \n",
    "    # post is used to store line by line info\n",
    "    post = []\n",
    "    # dic is used to store the overall info for one post\n",
    "    dic = {'text' : body, 'tokens' : [], 'anns' : []}\n",
    "    for index, value in enumerate(parts):\n",
    "        if code_or_not[index]:\n",
    "            dic, post = annotatet_code(value, dic, post)\n",
    "        else:\n",
    "            token = natural_lang_tokenizer.tokenize(value)\n",
    "            ann = nltk.pos_tag(token)\n",
    "            ann = [ann[i][1] for i in range(len(token))]\n",
    "            post.append([value, token, ann])\n",
    "            dic['tokens'] += token\n",
    "            dic['anns'] += ann\n",
    "    \n",
    "    \n",
    "    # df for each post, do line by line annotation\n",
    "    post_df = pd.DataFrame(post, columns = ['text', 'token', 'annotation'])\n",
    "    post_df.to_csv('../processed_data/annotations/'+str(post_number)+'.csv')\n",
    "    annotations.append([p_id, dic['text'], dic['tokens'], dic['anns']])\n",
    "    post_number += 1\n",
    "    \n",
    "    # only annotate 100 posts\n",
    "    if post_number == 100:\n",
    "        break\n",
    "# df for all posts\n",
    "results = pd.DataFrame(annotations, columns = ['post_id', 'text', 'tokens', 'annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.to_csv('../processed_data/ann_100_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
