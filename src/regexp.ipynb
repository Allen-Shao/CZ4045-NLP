{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "post_with_code = pd.read_csv(\"../processed_data/remove_duplicates_selected_codes.csv\")\n",
    "post_with_code.set_index('PostId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_code_blks = pd.read_csv(\"../processed_data/code_top300.csv\")\n",
    "selected_code_blks.set_index('PostId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "identifier = r'[a-zA-Z_][\\d\\w_]*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = r'''break|default|func|interface|select|case|defer\n",
    "                |go|map|struct|chan|else|goto|package|switch\n",
    "                |const|fallthrough|if|range|type|continue|for|import|return|var'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# operators and punctuation\n",
    "operators = r'[%/\\+\\-\\*\\,;\\$><!:\\.\\|&\\^=\\(\\)\\[\\]\\{\\}]+'\n",
    "\n",
    "logi_op = r'(%%|\\|\\||!)$' # %% || !\n",
    "arithmetic_op = r'(\\+|\\-|\\*|/|%|(\\+\\+)|(\\-\\-))$' # + = * / % ++ --\n",
    "bi_op = r'([\\|\\^&]|<<|>>)$' # | & ^ << >>\n",
    "assi_op = r'((:|\\+|\\-|\\*|/|%|(<<)|(>>)|&|\\^|\\|)?=)$' # = += -= *= /= %= <<= >>= &= ^= |= :=\n",
    "relation_op = r'(>|<|(>=)|(<=)|(==)|(!=))$'\n",
    "pointer_op = r'([&\\*])$'\n",
    "channel_op = r'<-$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decimal_literal = r'\\d+i?'\n",
    "octal_literal = r'0[1-7]*'\n",
    "hex_literal = r'0[xX][1-9a-fA-F]+'\n",
    "floating_literal = r''' \\d+\\.\\d*(?:[eE][+-]\\d+)?i?     \n",
    "                | \\d+[eE][+-]\\d+i?               \n",
    "                | \\.\\d+(?:[eE][+-]\\d+)?i?        \n",
    "                '''\n",
   "execution_count": null,
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = r'//.*$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "function_call = r'\\w+\\.\\w+\\s*\\(.*\\)'\n",
    "directory = r'(?:\\S+/)+\\S+/?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patterns =  keywords + '|' + function_call + '|' + string_literal + '|' + comments + '|'\\\n",
    "        + directory + '|' + identifier + '|' + operators + '|' \\\n",
    "        + hex_literal + '|' + floating_literal + '|' + octal_literal + '|' \\\n",
    "        + decimal_literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(patterns)\n",
    "natural_lang_tokenizer = RegexpTokenizer(r'\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"http.Handle('/view', appHandler(viewRecord))\",\n",
       " \"// don't require authentication\"]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = RegexpTokenizer(patterns)\n",
    "t = \"http.Handle('/view', appHandler(viewRecord))                  // don't require authentication\"\n",
    "test.tokenize(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = pd.read_csv('../processed_data/answers.csv')\n",
    "qns = pd.read_csv('../processed_data/questions.csv')\n",
    "answers.set_index('PostId', inplace=True)\n",
    "qns.set_index('PostId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def if_pointer(t, tokens):\n",
    "    if (re.match(pointer_op, t) is None):\n",
    "        return False\n",
    "    if (tokens.index(t) == len(tokens) - 1):\n",
    "#         if it is the last token, it will not be the pointer\n",
    "        return False\n",
    "    next_token = tokens[tokens.index(t) + 1]\n",
    "    if (re.match(identifier, next_token)):\n",
    "        if (tokens.index(t) == 0):\n",
    "#             if it is the first token\n",
    "            return True\n",
    "        else:\n",
    "            previous_token = tokens[tokens.index(t) - 1]\n",
    "            if (re.match(identifier, previous_token) is None):\n",
    "#                 if the previous token is not an identifier\n",
    "                return True\n",
    "    return False\n",
    "\n",
    "def operator_type(t, tokens):\n",
    "    if (if_pointer(t, tokens)):\n",
    "        return ('POINTER_OPERATOR')\n",
    "    elif re.match(logi_op, t):\n",
    "        return (\"LOGICAL_OPERATOR\")\n",
    "    elif re.match(arithmetic_op, t):\n",
    "        return (\"ARITHMETIC_OPERATOR\")\n",
    "    elif re.match(bi_op, t):\n",
    "        return (\"BITWISE_OPERATOR\")\n",
    "    elif re.match(assi_op, t):\n",
    "        return (\"ASSIGNMENT_OPERATOR\")\n",
    "    elif re.match(relation_op, t):\n",
    "        return (\"RELATION_OPERATOR\")\n",
    "    elif re.match(channel_op, t):\n",
    "        return (\"CHANNEL_OPERATOR\")\n",
    "    return (\"PUNCTUATION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def annotatet_code(code, dic, post):\n",
    "    code_lines = list(filter(None, code.split('\\n')))\n",
    "    for line in code_lines:\n",
    "        # store line info then append to 'post' array\n",
    "        line_info = [line]\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        line_info.append(tokens)\n",
    "        dic['tokens'] += (tokens)\n",
    "        # start annotation\n",
    "        ann = []\n",
    "        for t in tokens:\n",
    "            if re.match(comments, t):\n",
    "                ann.append('COMMENT')\n",
    "                t = t.replace('//', '')\n",
    "                # annotate comments as natural language\n",
    "                t = natural_lang_tokenizer.tokenize(t)\n",
    "                last = line_info.pop()\n",
    "                last.pop()\n",
    "                line_info.append(last + ['//'] + t)\n",
    "                dic['tokens'].pop()\n",
    "                dic['tokens'] += (last + ['//'] + t)\n",
    "                tags = nltk.pos_tag(t)\n",
    "                ann += [tags[i][1] for i in range(len(tags))]\n",
    "            elif re.match(keywords, t):\n",
    "                ann.append('KEYWORD')\n",
    "            elif re.match(boolean_literal, t):\n",
    "                ann.append('BOOLEAN_LITERAL')\n",
    "            elif re.match(function_call, t):\n",
    "                ann.append('FUNCTION_CALL')\n",
    "            elif re.match(string_literal, t):\n",
    "                ann.append('STRING_LITERAL')\n",
    "            elif re.match(directory, t):\n",
    "                ann.append('DIRECTORY')\n",
    "            elif re.match(identifier, t):\n",
    "                ann.append('IDENTIFIER')\n",
    "            elif re.match(operators, t):\n",
    "                ann.append(operator_type(t, tokens))\n",
    "            elif re.match(hex_literal, t):\n",
    "                ann.append('HEX_LITERAL')\n",
    "            elif re.match(floating_literal, t):\n",
    "                ann.append('FLOATING_LITERAL')\n",
    "            elif re.match(octal_literal, t):\n",
    "                ann.append('OCTAL_LITERAL')\n",
    "            elif re.match(decimal_literal, t):\n",
    "                ann.append('DECIMAL_LITERAL')\n",
    "            else:\n",
    "                ann.append('UNDEFINED')\n",
    "        line_info.append(ann)\n",
    "        dic['anns'] += (ann)\n",
    "        post.append(line_info)\n",
    "    return dic, post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# annotations is used to store all the 100 posts\n",
    "# each element in this array should be one particular post\n",
    "annotations = []\n",
    "post_number = 0\n",
    "# iterate through 150 posts in case there is any 'no match' problem\n",
    "# Note: the 'no-mathc' problem is due to different formatting/parsing methods from xml to csv\n",
    "for p_id in post_with_code.index[:150]:\n",
    "    no_match = False\n",
    "    if p_id in answers.index:\n",
    "        body = answers.loc[p_id]['Body']\n",
    "    elif p_id in qns.index:\n",
    "        body = qns.loc[p_id]['Body']\n",
    "    # separate code and natural language, store into parts\n",
    "    parts = []\n",
    "    # 1 for code, 0 for natural language\n",
    "    code_or_not = []\n",
    "    all_code_blks = selected_code_blks.loc[[p_id], 'Code']\n",
    "    position = {}\n",
    "    for code_blk in all_code_blks.values:\n",
    "        if body.find(code_blk) == -1:\n",
    "            print(str(p_id) + \" code not match body\")\n",
    "            no_match = True\n",
    "            break\n",
    "        if len(code_blk) > 20:\n",
    "            position[body.index(code_blk)] = code_blk \n",
    "    if no_match:\n",
    "        continue\n",
    "        \n",
    "    keylist = sorted(position.keys())\n",
    "    \n",
    "    for key in keylist:\n",
    "        code = position[key]\n",
    "        # handle the situation where the code appears more than once \n",
    "        # split based on fist occurence\n",
    "        rest = body.split(code, 1)\n",
    "        parts += ([rest[0], code])\n",
    "        code_or_not += ([0, 1] if rest[0] else [1])\n",
    "        if len(rest) > 2:\n",
    "            parts.append(rest[-1])\n",
    "            if rest[-1].strip():\n",
    "                code_or_not.append(0)\n",
    "        parts = list(filter(None, parts))\n",
    "        body = rest[-1]\n",
    "    if len(rest) == 2 and rest[1].strip():\n",
    "        parts.append(rest[1]) \n",
    "        code_or_not.append(0)\n",
    "    \n",
    "    # post is used to store line by line info\n",
    "    post = []\n",
    "    # dic is used to store the overall info for one post\n",
    "    dic = {'text' : body, 'tokens' : [], 'anns' : []}\n",
    "    for index, value in enumerate(parts):\n",
    "        if code_or_not[index]:\n",
    "            dic, post = annotatet_code(value, dic, post)\n",
    "        else:\n",
    "            token = natural_lang_tokenizer.tokenize(value)\n",
    "            ann = nltk.pos_tag(token)\n",
    "            ann = [ann[i][1] for i in range(len(token))]\n",
    "            post.append([value, token, ann])\n",
    "            dic['tokens'] += token\n",
    "            dic['anns'] += ann\n",
    "    \n",
    "    \n",
    "    # df for each post, do line by line annotation\n",
    "    post_df = pd.DataFrame(post, columns = ['text', 'token', 'annotation'])\n",
    "    post_df.to_csv('../processed_data/annotations/'+str(post_number)+'.csv')\n",
    "    annotations.append([p_id, dic['text'], dic['tokens'], dic['anns']])\n",
    "    post_number += 1\n",
    "    \n",
    "    # only annotate 100 posts\n",
    "    if post_number == 5:\n",
    "        break\n",
    "# df for all posts\n",
    "results = pd.DataFrame(annotations, columns = ['post_id', 'text', 'tokens', 'annotations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "results.to_csv('../processed_data/ann_100_results.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
