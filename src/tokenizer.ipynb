{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_code(code, dic, post):\n",
    "    code_lines = list(filter(None, code.split('\\n')))\n",
    "    for line in code_lines:\n",
    "        # store line info then append to 'post' array\n",
    "        line_info = [line]\n",
    "        tokens = tokenizer.tokenize(line)\n",
    "        line_info.append(tokens)\n",
    "        dic['tokens'] += (tokens)\n",
    "        if tokens and re.match(comments, tokens[0]):\n",
    "            t = tokens[0]\n",
    "            t = t.replace('//', '')\n",
    "            # tokenize comments as natural language\n",
    "            t = natural_lang_tokenizer.tokenize(t)\n",
    "            last = line_info.pop()\n",
    "            last.pop()\n",
    "            line_info.append(last + ['//'] + t)\n",
    "            \n",
    "            dic['tokens'].pop()\n",
    "            dic['tokens'] += (last + ['//'] + t)\n",
    "        post.append(line_info)\n",
    "    return dic, post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _tokenizer(post_df, code_df, store_directory, store_each_post = False, process_amount = None):\n",
    "    tokenization=[]\n",
    "    count = 0\n",
    "    for post_number,p_id in enumerate(post_df.index.values):\n",
    "        no_match = False\n",
    "        body = post_df.loc[p_id]['Body']\n",
    "        # separate code and natural language, store into parts\n",
    "        parts = []\n",
    "        # 1 for code, 0 for natural language\n",
    "        code_or_not = []\n",
    "        # if the body does not have any code block, no need to process\n",
    "        if p_id not in code_df.index:\n",
    "            parts.append(body)\n",
    "            code_or_not=[0]\n",
    "        else:\n",
    "            code_blks = code_df.loc[p_id]['Code']\n",
    "            # check code blk position in a post\n",
    "            # arrange them in order\n",
    "            position = {}\n",
    "            if isinstance(code_blks, str):\n",
    "                if body.find(code_blks) == -1:\n",
    "                    no_match = True\n",
    "                elif len(code_blks) > 20:\n",
    "                    position[body.index(code_blks)] = code_blks\n",
    "            else:\n",
    "                for code_blk in code_blks.values:\n",
    "                    # Note: the 'no-match' problem is due to different formatting/parsing methods from xml to csv\n",
    "                    if not isinstance(code_blk, str):\n",
    "                        continue\n",
    "                    if body.find(code_blk) == -1:\n",
    "                        no_match = True\n",
    "                        break\n",
    "                    if len(code_blk) > 20:\n",
    "                        position[body.index(code_blk)] = code_blk \n",
    "            if no_match:\n",
    "                continue\n",
    "            keylist = sorted(position.keys())\n",
    "            for key in keylist:\n",
    "                code = position[key]\n",
    "                # handle the situation where the code appears more than once \n",
    "                # split based on fist occurence\n",
    "                rest = body.split(code, 1)\n",
    "                parts += ([rest[0], code])\n",
    "                code_or_not += ([0, 1] if rest[0] else [1])\n",
    "                if len(rest) > 2:\n",
    "                    parts.append(rest[-1])\n",
    "                    if rest[-1].strip():\n",
    "                        code_or_not.append(0)\n",
    "\n",
    "                parts = list(filter(None, parts))\n",
    "                body = rest[-1]\n",
    "            if len(rest) == 2 and rest[1].strip():\n",
    "                parts.append(rest[1]) \n",
    "                code_or_not.append(0)\n",
    "        # post is used to store line by line info\n",
    "        post = []\n",
    "        # dic is used to store the overall info for one post\n",
    "        dic = {'text' : body, 'tokens' : []}\n",
    "        for index, value in enumerate(parts):\n",
    "            if code_or_not[index]:\n",
    "                dic, post = tokenize_code(value, dic, post)\n",
    "            else:\n",
    "                token = natural_lang_tokenizer.tokenize(value)\n",
    "                post.append([value, token])\n",
    "                dic['tokens'] += token\n",
    "\n",
    "        # df for each post, do line by line annotation\n",
    "        if store_each_post:\n",
    "            post_tokenization_df = pd.DataFrame(post, columns = ['text', 'token'])\n",
    "            post_tokenization_df.to_csv(store_directory + str(post_number)+'.csv')\n",
    "        tokenization.append([p_id, dic['text'], dic['tokens']])\n",
    "        count +=1\n",
    "        if process_amount and count >= process_amount:\n",
    "            break\n",
    "\n",
    "    # df for all posts\n",
    "    results = pd.DataFrame(tokenization, columns = ['post_id', 'text', 'tokens'])\n",
    "    results.to_csv(store_directory + 'overall_results.csv')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "answers = pd.read_csv('../processed_data/answers.csv')\n",
    "qns = pd.read_csv('../processed_data/questions.csv')\n",
    "answers.set_index('ParentId', inplace=True)\n",
    "qns.set_index('PostId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_code_blks = pd.read_csv(\"../processed_data/code_top1000.csv\")\n",
    "selected_code_blks.set_index('PostId', inplace=True)\n",
    "selected_post_ids = selected_code_blks.index.drop_duplicates(keep='first')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_codes = pd.read_csv(\"../processed_data/code.csv\")\n",
    "ALL_codes.set_index('PostId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all posts(include ans and qns) from around 700 selected threads\n",
    "selected_post_list = []\n",
    "selected_qns_list = []\n",
    "thread_count = 0\n",
    "no_answer = 0\n",
    "for p_id in selected_post_ids:\n",
    "    if p_id in qns.index.values:\n",
    "        thread_count += 1\n",
    "        selected_post_list.append([p_id, qns.loc[p_id]['Body']])\n",
    "        selected_qns_list.append([p_id, qns.loc[p_id]['Body']])\n",
    "        # continue if the question do not have any answer\n",
    "        if not qns.loc[p_id]['AnswerCount']:\n",
    "            no_answer += 1\n",
    "            continue\n",
    "        # else, add answers to answer list\n",
    "        ans = answers.loc[p_id].values\n",
    "        if ans.ndim == 1:\n",
    "            selected_post_list.append([ans[1], ans[2]])\n",
    "        else:\n",
    "            for a in answers.loc[p_id].values:\n",
    "                selected_post_list.append([a[1], a[2]])\n",
    "        if thread_count > 700:\n",
    "            break\n",
    "print(str(no_answer) + \" of questions does not have answer\")\n",
    "selected_post = pd.DataFrame(selected_post_list, columns=['PostId', 'Body'])  \n",
    "selected_post.set_index('PostId', inplace=True)  \n",
    "selected_qns = pd.DataFrame(selected_post_list, columns=['PostId', 'Body'])  \n",
    "selected_qns.set_index('PostId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_post = pd.read_csv('../processed_data/150_ground_truth_post.csv')\n",
    "ground_truth_post.set_index('PostId', inplace=True)\n",
    "ground_truth_post_ids = ground_truth_post.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groud_truth_code_blk_list=[]\n",
    "n = 0\n",
    "for p_id in ground_truth_post_ids:\n",
    "    if p_id in ALL_codes.index:\n",
    "        groud_truth_code_blk_list.append([p_id, ALL_codes.loc[p_id]['Code']])\n",
    "    else:\n",
    "        n += 1\n",
    "print(str(n) + \" number of posts do not have code block\")\n",
    "ground_truth_code_blk = pd.DataFrame(groud_truth_code_blk_list, columns=['PostId', 'Code'])\n",
    "ground_truth_code_blk.set_index('PostId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "identifier = r'[a-zA-Z_][\\d\\w_]*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = r'''break|default|func|interface|select|case|defer\n",
    "                |go|map|struct|chan|else|goto|package|switch\n",
    "                |const|fallthrough|if|range|type|continue|for|import|return|var'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# operators and punctuation\n",
    "operators = r'[%/\\+\\-\\*\\,;\\$><!:\\.\\|&\\^=\\(\\)\\[\\]\\{\\}]+'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decimal_literal = r'\\d+i?'\n",
    "octal_literal = r'0[1-7]*'\n",
    "hex_literal = r'0[xX][1-9a-fA-F]+'\n",
    "floating_literal = r''' \\d+\\.\\d*(?:[eE][+-]\\d+)?i?     \n",
    "                | \\d+[eE][+-]\\d+i?               \n",
    "                | \\.\\d+(?:[eE][+-]\\d+)?i?        \n",
    "                '''\n",
    "string_literal = r'''(?:\\\"\\s*.*?\\n?\\\")|(?:\\'\\s*.*?\\n?\\')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = r'//.*$'\n",
    "function_call = r'\\w+\\.\\w+\\s*\\(.*\\)'\n",
    "directory = r'/?\\w+/(?:.+/)*\\S+/?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patterns =  keywords + '|' + function_call + '|' + string_literal + '|' + comments + '|'\\\n",
    "        + directory + '|' + identifier + '|' + operators + '|' \\\n",
    "        + hex_literal + '|' + floating_literal + '|' + octal_literal + '|' \\\n",
    "        + decimal_literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(patterns)\n",
    "natural_lang_tokenizer = RegexpTokenizer(r'\\S+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize all ground truth posts\n",
    "gt_results = _tokenizer(ground_truth_post, ground_truth_code_blk, '../processed_data/tokenizer_result/gt_results/', True, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize all posts from 700 threads\n",
    "all_results = _tokenizer(selected_post, selected_code_blks , '../processed_data/tokenizer_result/others/', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
