{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "import numpy as np\n",
    "import string\n",
    "from nltk import ne_chunk, pos_tag, word_tokenize\n",
    "from nltk.tree import Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize_code(code, dic, post):\n",
    "    code_lines = list(filter(None, code.split('\\n')))\n",
    "    for line in code_lines:\n",
    "        # store line info then append to 'post' array\n",
    "        line_info = [line]\n",
    "        tokens = code_tokenizer.tokenize(line)  \n",
    "        # remove function definition from matching the case of function call\n",
    "        fun_def = False\n",
    "        for t in tokens:\n",
    "            if re.match(function_call, t) and not re.match(r'(?:\\w+\\.\\w+\\s*\\(.*\\))',t):\n",
    "                # if there is a { symbol after a function\n",
    "                # re-tokenize the line\n",
    "                if t != tokens[-1] and '{' in tokens[tokens.index(t)+1:]:\n",
    "                    fun_def = True\n",
    "                    break\n",
    "        if fun_def:\n",
    "            temp_patterns = patterns =  keywords + '|' + string_literal + '|' + comments + '|'\\\n",
    "                + directory + '|' + identifier + '|' + floating_literal + '|' \\\n",
    "                + hex_literal + '|' + operators + '|' + octal_literal + '|' \\\n",
    "                + decimal_literal\n",
    "            temp_code_tokenizer = RegexpTokenizer(temp_patterns)\n",
    "            tokens = temp_code_tokenizer.tokenize(line)\n",
    "        line_info.append(tokens)\n",
    "        dic['tokens'] += (tokens)\n",
    "        if tokens and re.match(comments, tokens[-1]):\n",
    "            t = tokens[-1]\n",
    "            t = t.replace('//', '')\n",
    "            # tokenize comments as natural language\n",
    "            t = natural_language_tokenizer(t)\n",
    "            last = line_info.pop()\n",
    "            last.pop()\n",
    "            line_info.append(last + ['//'] + t)\n",
    "            \n",
    "            dic['tokens'].pop()\n",
    "            dic['tokens'] += (last + ['//'] + t)\n",
    "        post.append(line_info)\n",
    "    return dic, post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_punc(token):\n",
    "    if token == '...':\n",
    "        return [token, token]\n",
    "    punc = string.punctuation.replace('/', '')\n",
    "    punctuation_mark = r\"[{}]\".format(punc) # create the pattern\n",
    "    left = '<{[('\n",
    "    right = '>}])'\n",
    "    if re.match(punctuation_mark, token[0]) and not re.match(punctuation_mark, token[-1]):\n",
    "#         print(\"first char is punctuation mark\")\n",
    "        token_lst = []\n",
    "        iter_token = iter(range(len(token)))\n",
    "        for i in iter_token: \n",
    "            if token[i] in punc:\n",
    "                if token[i] in left:\n",
    "                    corresponding_right = right[left.index(token[i])]\n",
    "                    if corresponding_right in token:\n",
    "                        token_lst.append(token[i:])\n",
    "                        token_lst.insert(0, token[i:])\n",
    "                        break\n",
    "                token_lst.append(token[i])\n",
    "            else:\n",
    "                token_lst.append(token[i:])\n",
    "                # copy the word token in front for later processing\n",
    "                token_lst.insert(0, token[i:])\n",
    "                break\n",
    "        return token_lst\n",
    "    elif re.match(punctuation_mark, token[-1]) and not re.match(punctuation_mark, token[0]):\n",
    "#         print(\"last char is punctuation mark\")\n",
    "        token_lst = []\n",
    "        iter_token = iter(range(len(token)-1, -1, -1))\n",
    "        for i in iter_token:\n",
    "            if token[i] in punc:\n",
    "                \n",
    "                if token[i] in right:\n",
    "                    corresponding_left = left[right.index(token[i])]\n",
    "                    if corresponding_left in token:\n",
    "                        token_lst.insert(0, token[:i+1])\n",
    "                        token_lst.insert(0, token[:i+1])\n",
    "                        break\n",
    "                token_lst.insert(0, token[i])\n",
    "            else:\n",
    "                token_lst.insert(0, token[:i+1])\n",
    "                # copy the word token in front for later processing\n",
    "                token_lst.insert(0, token[:i+1])\n",
    "                break\n",
    "                \n",
    "        return token_lst\n",
    "    else:\n",
    "        return [token, token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def natural_language_tokenizer(txt):\n",
    "#     re_entity = r'(?:[A-Z][A-Za-z]*\\s)*[A-Z][A-Za-z]*'\n",
    "    \n",
    "#     entities = RegexpTokenizer(re_entity).tokenize(txt)\n",
    "#     name_entities = []\n",
    "#     for entity in entities:\n",
    "#         if \" \" not in entity:\n",
    "#             break\n",
    "#         satisfier = True\n",
    "#         pos = nltk.pos_tag(entity.split(\" \"))[0][1]:\n",
    "#         if pos == 'DT' or pos == 'PRP'or pos == 'CC' or pos == 'WRB' or pos == 'IN':\n",
    "#             satisfier = False\n",
    "#             break\n",
    "#         if satisfier:    \n",
    "#             name_entities.append(entity)\n",
    "            \n",
    "#     if name_entities:\n",
    "#         print(name_entities)\n",
    "#     dic = {entity:entity.split(\" \") for entity in name_entities}\n",
    "#     entity_tokens = [item for sublist in dic.values() for item in sublist]\n",
    "    space_tokenizer = RegexpTokenizer(r'\\S+')\n",
    "    tokens = space_tokenizer.tokenize(txt)\n",
    "    \n",
    "    new_token_list = []\n",
    "#     find_entity = False\n",
    "    iter_tokens = iter(tokens)\n",
    "    for token in iter_tokens:\n",
    "#         for key, val in dic.items():\n",
    "#             index = tokens.index(token)\n",
    "#             if token in val and (index+len(val)) < len(tokens):\n",
    "#                 reconstruct = \" \".join([tokens[t] for t in range(index, index + len(val))])\n",
    "#                 splited = split_punc(reconstruct)\n",
    "#                 if splited[0] == key:\n",
    "#                     new_token_list += splited[1:]\n",
    "#                     if len(val) != 1:\n",
    "#                         for n in range(len(val)-1):\n",
    "#                             next(iter_tokens) \n",
    "#                     find_entity = True\n",
    "#                     break\n",
    "#         if find_entity:\n",
    "#             find_entity = False\n",
    "#             continue\n",
    "        \n",
    "        new_token_list += split_punc(token)[1:]\n",
    "       \n",
    "    # split abbreviation\n",
    "    abbreviation = r'(?:n\\'t$)|(?:\\'s$)|(?:\\'m$)|(?:\\'ve$)|(?:\\'re$)|(?:\\'ll$)|(?:\\'d$)'\n",
    "    old_tokens = new_token_list[:]\n",
    "    match_count = 0\n",
    "    for i, t in enumerate(old_tokens):\n",
    "        if re.search(abbreviation, t):\n",
    "            m = re.search(abbreviation, t)\n",
    "            word_root = t[:m.span()[0]]\n",
    "            suffix = t[m.span()[0]:m.span()[1]]\n",
    "            new_token_list[i+match_count] = word_root\n",
    "            new_token_list.insert(i+match_count+1, suffix)\n",
    "            match_count += 1\n",
    "            \n",
    "    return new_token_list   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _tokenizer(post_df, code_df, store_directory, store_each_post = False, process_amount = None):\n",
    "    tokenization=[]\n",
    "    count = 0\n",
    "    for post_number,p_id in enumerate(post_df.index.values):\n",
    "        no_match = False\n",
    "        body = post_df.loc[p_id]['Body']\n",
    "        # dic is used to store the overall info for one post\n",
    "        dic = {'text' : body, 'tokens' : []}\n",
    "        # separate code and natural language, store into parts\n",
    "        parts = []\n",
    "        # 1 for code, 0 for natural language\n",
    "        code_or_not = []\n",
    "        # if the body does not have any code block, no need to process\n",
    "        if p_id in code_df.index:\n",
    "            code_blks = code_df.loc[p_id]['Code']\n",
    "            # check code blk position in a post\n",
    "            # arrange them in order\n",
    "            position = {}\n",
    "            if isinstance(code_blks, str):\n",
    "                if body.find(code_blks) == -1:\n",
    "                    no_match = True\n",
    "                elif len(code_blks) > 20:\n",
    "                    position[body.index(code_blks)] = code_blks\n",
    "            else:\n",
    "                for code_blk in code_blks.values:\n",
    "                    # Note: the 'no-match' problem is due to different formatting/parsing methods from xml to csv\n",
    "                    if not isinstance(code_blk, str):\n",
    "                        continue\n",
    "                    if body.find(code_blk) == -1:\n",
    "                        no_match = True\n",
    "                        break\n",
    "                    if len(code_blk) > 20:\n",
    "                        position[body.index(code_blk)] = code_blk \n",
    "            if no_match:\n",
    "                continue\n",
    "            keylist = sorted(position.keys())\n",
    "            for key in keylist:\n",
    "                code = position[key]\n",
    "                # handle the situation where the code appears more than once \n",
    "                # split based on fist occurence\n",
    "                rest = body.split(code, 1)\n",
    "                parts += ([rest[0], code])\n",
    "                code_or_not += ([0, 1] if rest[0] else [1])\n",
    "                if len(rest) > 2:\n",
    "                    parts.append(rest[-1])\n",
    "                    if rest[-1].strip():\n",
    "                        code_or_not.append(0)\n",
    "\n",
    "                parts = list(filter(None, parts))\n",
    "                body = rest[-1]\n",
    "            if keylist and len(rest) == 2 and rest[1].strip():\n",
    "                parts.append(rest[1]) \n",
    "                code_or_not.append(0)\n",
    "        # if there is not code block\n",
    "        if not parts:\n",
    "            parts.append(body)\n",
    "            code_or_not=[0]\n",
    "        # post is used to store line by line info\n",
    "        post = []\n",
    "        for index, value in enumerate(parts):\n",
    "            if code_or_not[index]:\n",
    "                dic, post = tokenize_code(value, dic, post)\n",
    "            else:\n",
    "                token = natural_language_tokenizer(value)\n",
    "                post.append([value, token])\n",
    "                dic['tokens'] += token\n",
    "        # df for each post, do line by line annotation\n",
    "        if store_each_post:\n",
    "            post_tokenization_df = pd.DataFrame(post, columns = ['text', 'token'])\n",
    "            post_tokenization_df.to_csv(store_directory + str(post_number) + \".csv\")\n",
    "        tokenization.append([p_id, dic['text'], dic['tokens']])\n",
    "        count +=1\n",
    "        if process_amount and count >= process_amount:\n",
    "            break\n",
    "    # df for all posts\n",
    "    results = pd.DataFrame(tokenization, columns = ['post_id', 'text', 'tokens'])\n",
    "    results.to_csv(store_directory + 'overall_results.csv')\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_continuous_chunks(text):\n",
    "    chunked = ne_chunk(pos_tag(word_tokenize(text)))\n",
    "    prev = None\n",
    "    continuous_chunk = []\n",
    "    current_chunk = []\n",
    "\n",
    "    for i in chunked:\n",
    "        if type(i) == Tree:\n",
    "            current_chunk.append(\" \".join([token for token, pos in i.leaves()]))\n",
    "        elif current_chunk:\n",
    "            named_entity = \" \".join(current_chunk)\n",
    "            if named_entity not in continuous_chunk:\n",
    "                continuous_chunk.append(named_entity)\n",
    "                current_chunk = []\n",
    "        else:\n",
    "            continue\n",
    "    # capture the ones at the end of the sentense\n",
    "    if current_chunk:\n",
    "        named_entity = \" \".join(current_chunk)\n",
    "        if named_entity not in continuous_chunk:\n",
    "            continuous_chunk.append(named_entity)\n",
    "            current_chunk = []\n",
    "    return continuous_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def natural_language_tokenizer_use_chunck(txt):\n",
    "    \n",
    "    name_entities = get_continuous_chunks(txt)\n",
    "    dic = {entity:entity.split(\" \") for entity in name_entities}\n",
    "    entity_tokens = [item for sublist in dic.values() for item in sublist]\n",
    "    \n",
    "    space_tokenizer = RegexpTokenizer(r'\\S+')\n",
    "    tokens = space_tokenizer.tokenize(txt)\n",
    "    \n",
    "    new_token_list = []\n",
    "    find_entity = False\n",
    "    iter_tokens = iter(tokens)\n",
    "    for token in iter_tokens:\n",
    "        for key, val in dic.items():\n",
    "            index = tokens.index(token)\n",
    "            if token in val:\n",
    "                reconstruct = \" \".join([tokens[t] for t in range(index, index + len(val))])\n",
    "                splited = split_punc(reconstruct)\n",
    "                if splited[0] == key:\n",
    "                    new_token_list += splited[1:]\n",
    "                    if len(val) != 1:\n",
    "                        for n in range(len(val)):\n",
    "                            next(iter_tokens) \n",
    "                    find_entity = True\n",
    "                    break\n",
    "        if find_entity:\n",
    "            find_entity = False\n",
    "            continue\n",
    "        new_token_list += split_punc(token)[1:]\n",
    "        \n",
    "    return new_token_list     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ALL_codes = pd.read_csv(\"../processed_data/code.csv\")\n",
    "ALL_codes.set_index('PostId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_qns = pd.read_csv('../processed_data/selected_questions.csv')\n",
    "selected_ans = pd.read_csv('../processed_data/selected_answer.csv')\n",
    "selected_post = pd.concat([selected_ans, selected_qns])[['PostId', 'Body']]\n",
    "selected_post.set_index('PostId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ground_truth_post = pd.read_csv('../processed_data/150_ground_truth_post.csv')\n",
    "ground_truth_post.set_index('PostId', inplace=True)\n",
    "ground_truth_post_ids = ground_truth_post.index.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groud_truth_code_blk_list=[]\n",
    "n = 0\n",
    "for p_id in ground_truth_post_ids:\n",
    "    if p_id in ALL_codes.index:\n",
    "        groud_truth_code_blk_list.append([p_id, ALL_codes.loc[p_id]['Code']])\n",
    "    else:\n",
    "        n += 1\n",
    "print(str(n) + \" number of posts do not have code block\")\n",
    "ground_truth_code_blk = pd.DataFrame(groud_truth_code_blk_list, columns=['PostId', 'Code'])\n",
    "ground_truth_code_blk.set_index('PostId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "identifier = r'[a-zA-Z_][\\d\\w_]*'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keywords = r'''break|default|func|interface|select|case|defer\n",
    "                |go|map|struct|chan|else|goto|package|switch\n",
    "                |const|fallthrough|if|range|type|continue|for|import|return|var'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# operators and punctuation\n",
    "operators = r'[%/\\+\\-\\*\\,;\\$><!:\\.\\|&\\^=\\(\\)\\[\\]\\{\\}]+'\n",
    "\n",
    "# special symbol\n",
    "special = r'[#]'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "decimal_literal = r'\\d+i?'\n",
    "octal_literal = r'0[1-7]*'\n",
    "hex_literal = r'0[xX][0-9a-fA-F]+'\n",
    "floating_literal = r'''(?:\\d+\\.\\d*(?:[eE][+-]\\d+)?i?)|(?:\\d+[eE][+-]\\d+i?)|(?:\\.\\d+(?:[eE][+-]\\d+)?i?)'''\n",
    "string_literal = r'''(?:\\\"\\s*.*?\\n?\\\")|(?:\\'\\s*.*?\\n?\\')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "comments = r'//.*$'\n",
    "function_call = r'(?:\\w+\\.\\w+\\s*\\(.*\\))|(?:\\w+\\s*\\(.*\\))'\n",
    "directory = r'/?\\w+/(?:.+/)*\\S+/?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patterns = special + '|' + keywords + '|' + function_call + '|' + string_literal + '|' + comments + '|'\\\n",
    "        + directory + '|' + identifier + '|' + floating_literal + '|' \\\n",
    "        + hex_literal + '|' + operators + '|' + octal_literal + '|' \\\n",
    "        + decimal_literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "code_tokenizer = RegexpTokenizer(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tokenize all ground truth posts\n",
    "gt_results = _tokenizer(ground_truth_post, ground_truth_code_blk, '../processed_data/tokenizer_result/generated_100_result_v2/', True, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tokenize all posts from 700 threads\n",
    "all_results = _tokenizer(selected_post, ALL_codes , '../processed_data/tokenizer_result/all/', False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
