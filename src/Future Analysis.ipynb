{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Irregular Tokens. Tokenize the dataset using your own tokenizer. Identify the top-20 most frequent\n",
    "# tokens which are NOT standard English words (including their morphological forms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enchant\n",
    "import pandas as pd\n",
    "import os.path as path\n",
    "import ast\n",
    "import operator\n",
    "import re\n",
    "from collections import Counter\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "import nltk\n",
    "import string\n",
    "from nltk import pos_tag, word_tokenize\n",
    "import argparse\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_code(code):\n",
    "    code_lines = list(filter(None, code.split('\\n')))\n",
    "    for line in code_lines:\n",
    "        # store line info then append to 'post' array\n",
    "        line_info = [line]\n",
    "        tokens = code_tokenizer.tokenize(line)\n",
    "        # remove function definition from matching the case of function call\n",
    "        fun_def = False\n",
    "        for t in tokens:\n",
    "            if re.match(function_call, t) and not re.match(r'(?:\\w+\\.\\w+\\s*\\(.*\\))',t):\n",
    "                # if there is a { symbol after a function\n",
    "                # re-tokenize the line\n",
    "                if t != tokens[-1] and '{' in tokens[tokens.index(t)+1:]:\n",
    "                    fun_def = True\n",
    "                    break\n",
    "        if fun_def:\n",
    "            temp_patterns = patterns =  keywords + '|' + string_literal + '|' + comments + '|'\\\n",
    "                + directory + '|' + identifier + '|' + floating_literal + '|' \\\n",
    "                + hex_literal + '|' + operators + '|' + octal_literal + '|' \\\n",
    "                + decimal_literal\n",
    "            temp_code_tokenizer = RegexpTokenizer(temp_patterns)\n",
    "            tokens = temp_code_tokenizer.tokenize(line)\n",
    "        line_info.append(tokens)\n",
    "\n",
    "        if tokens and re.match(comments, tokens[-1]):\n",
    "            t = tokens[-1]\n",
    "            t = t.replace('//', '')\n",
    "            # tokenize comments as natural language\n",
    "            t = tokenize_natural_language(t)\n",
    "            last = line_info.pop()\n",
    "            last.pop()\n",
    "            line_info.append(last + ['//'] + t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def split_punc(token):\n",
    "\n",
    "    punc = string.punctuation.replace('/', '')\n",
    "    punctuation_mark = r\"[{}]\".format(punc) # create the pattern\n",
    "    left = '<{[('\n",
    "    right = '>}])'\n",
    "    if re.match(punctuation_mark, token[0]) and not re.match(punctuation_mark, token[-1]):\n",
    "#         print(\"first char is punctuation mark\")\n",
    "        token_lst = []\n",
    "        iter_token = iter(range(len(token)))\n",
    "        for i in iter_token: \n",
    "            if token[i] in punc:\n",
    "                if token[i] in left:\n",
    "                    corresponding_right = right[left.index(token[i])]\n",
    "                    if corresponding_right in token:\n",
    "                        token_lst.append(token[i:])\n",
    "                        token_lst.insert(0, token[i:])\n",
    "                        break\n",
    "                token_lst.append(token[i])\n",
    "            else:\n",
    "                token_lst.append(token[i:])\n",
    "                # copy the word token in front for later processing\n",
    "                token_lst.insert(0, token[i:])\n",
    "                break\n",
    "        omit = ''\n",
    "        lst = token_lst[:]\n",
    "        for i in range(len(lst)-1, -1, -1):\n",
    "            if token_lst[i] == '.':\n",
    "                omit += token_lst[i]\n",
    "            else:\n",
    "                if len(omit) >= 3:\n",
    "                    token_lst = token_lst[:i+1]\n",
    "                    token_lst.append(omit)\n",
    "                    if (i+len(omit)+1) < len(lst):\n",
    "                        token_lst += lst[i+len(omit)+1:]\n",
    "                    break\n",
    "        return token_lst\n",
    "    elif re.match(punctuation_mark, token[-1]) and not re.match(punctuation_mark, token[0]):\n",
    "#         print(\"last char is punctuation mark\")\n",
    "        token_lst = []\n",
    "        iter_token = iter(range(len(token)-1, -1, -1))\n",
    "        for i in iter_token:\n",
    "            if token[i] in punc:\n",
    "                \n",
    "                if token[i] in right:\n",
    "                    corresponding_left = left[right.index(token[i])]\n",
    "                    if corresponding_left in token:\n",
    "                        token_lst.insert(0, token[:i+1])\n",
    "                        token_lst.insert(0, token[:i+1])\n",
    "                        break\n",
    "                token_lst.insert(0, token[i])\n",
    "            else:\n",
    "                token_lst.insert(0, token[:i+1])\n",
    "                # copy the word token in front for later processing\n",
    "                token_lst.insert(0, token[:i+1])\n",
    "                break\n",
    "        omit = ''\n",
    "        lst = token_lst[:]\n",
    "        for i in range(len(lst)-1, -1, -1):\n",
    "            if token_lst[i] == '.':\n",
    "                omit += token_lst[i]\n",
    "            else:\n",
    "                if len(omit) >= 3:\n",
    "                    token_lst = token_lst[:i+1]\n",
    "                    token_lst.append(omit)\n",
    "                    if (i+len(omit)+1) < len(lst):\n",
    "                        token_lst += lst[i+len(omit)+1:]\n",
    "                    break        \n",
    "        return token_lst\n",
    "    else:\n",
    "        return [token, token]\n",
    "\n",
    "def tokenize_natural_language(txt):\n",
    "#     re_entity = r'(?:[A-Z][A-Za-z]*\\s)*[A-Z][A-Za-z]*'\n",
    "\n",
    "#     entities = RegexpTokenizer(re_entity).tokenize(txt)\n",
    "#     name_entities = []\n",
    "#     for entity in entities:\n",
    "#         if \" \" not in entity:\n",
    "#             break\n",
    "#         satisfier = True\n",
    "#         pos = nltk.pos_tag(entity.split(\" \"))[0][1]:\n",
    "#         if pos == 'DT' or pos == 'PRP'or pos == 'CC' or pos == 'WRB' or pos == 'IN':\n",
    "#             satisfier = False\n",
    "#             break\n",
    "#         if satisfier:\n",
    "#             name_entities.append(entity)\n",
    "\n",
    "#     if name_entities:\n",
    "#         print(name_entities)\n",
    "#     dic = {entity:entity.split(\" \") for entity in name_entities}\n",
    "#     entity_tokens = [item for sublist in dic.values() for item in sublist]\n",
    "    space_tokenizer = RegexpTokenizer(r'\\S+')\n",
    "    tokens = space_tokenizer.tokenize(txt)\n",
    "\n",
    "    new_token_list = []\n",
    "#     find_entity = False\n",
    "    iter_tokens = iter(tokens)\n",
    "    for token in iter_tokens:\n",
    "#         for key, val in dic.items():\n",
    "#             index = tokens.index(token)\n",
    "#             if token in val and (index+len(val)) < len(tokens):\n",
    "#                 reconstruct = \" \".join([tokens[t] for t in range(index, index + len(val))])\n",
    "#                 splited = split_punc(reconstruct)\n",
    "#                 if splited[0] == key:\n",
    "#                     new_token_list += splited[1:]\n",
    "#                     if len(val) != 1:\n",
    "#                         for n in range(len(val)-1):\n",
    "#                             next(iter_tokens)\n",
    "#                     find_entity = True\n",
    "#                     break\n",
    "#         if find_entity:\n",
    "#             find_entity = False\n",
    "#             continue\n",
    "\n",
    "        new_token_list += split_punc(token)[1:]\n",
    "\n",
    "    # split abbreviation\n",
    "    abbreviation = r'(?:n\\'t$)|(?:\\'s$)|(?:\\'m$)|(?:\\'ve$)|(?:\\'re$)|(?:\\'ll$)|(?:\\'d$)'\n",
    "    old_tokens = new_token_list[:]\n",
    "    match_count = 0\n",
    "    for i, t in enumerate(old_tokens):\n",
    "        if re.search(abbreviation, t):\n",
    "            m = re.search(abbreviation, t)\n",
    "            word_root = t[:m.span()[0]]\n",
    "            suffix = t[m.span()[0]:m.span()[1]]\n",
    "            new_token_list[i+match_count] = word_root\n",
    "            new_token_list.insert(i+match_count+1, suffix)\n",
    "            match_count += 1\n",
    "\n",
    "    return new_token_list\n",
    "\n",
    "def differentiate_code_nl(p_id, body, code_df):\n",
    "    tokenization=[]\n",
    "    count = 0\n",
    "\n",
    "    no_match = False\n",
    "    # dic is used to store the overall info for one post\n",
    "    dic = {'text' : body, 'tokens' : []}\n",
    "    # separate code and natural language, store into parts\n",
    "    parts = []\n",
    "    # 1 for code, 0 for natural language\n",
    "    code_or_not = []\n",
    "    # if the body does not have any code block, no need to process\n",
    "    if p_id in code_df.index:\n",
    "        code_blks = code_df.loc[p_id]['Code']\n",
    "        # check code blk position in a post\n",
    "        # arrange them in order\n",
    "        position = {}\n",
    "        if isinstance(code_blks, str):\n",
    "            if body.find(code_blks) == -1:\n",
    "                no_match = True\n",
    "            elif len(code_blks) > 20:\n",
    "                position[body.index(code_blks)] = code_blks\n",
    "        else:\n",
    "            for code_blk in code_blks.values:\n",
    "                # Note: the 'no-match' problem is due to different formatting/parsing methods from xml to csv\n",
    "                if not isinstance(code_blk, str):\n",
    "                    continue\n",
    "                if body.find(code_blk) == -1:\n",
    "                    no_match = True\n",
    "                    break\n",
    "                if len(code_blk) > 20:\n",
    "                    position[body.index(code_blk)] = code_blk\n",
    "        if no_match:\n",
    "            return None\n",
    "        keylist = sorted(position.keys())\n",
    "        for key in keylist:\n",
    "            code = position[key]\n",
    "            # handle the situation where the code appears more than once\n",
    "            # split based on fist occurence\n",
    "            rest = body.split(code, 1)\n",
    "            parts += ([rest[0], code])\n",
    "            code_or_not += ([0, 1] if rest[0] else [1])\n",
    "            if len(rest) > 2:\n",
    "                parts.append(rest[-1])\n",
    "                if rest[-1].strip():\n",
    "                    code_or_not.append(0)\n",
    "\n",
    "            parts = list(filter(None, parts))\n",
    "            body = rest[-1]\n",
    "        if keylist and len(rest) == 2 and rest[1].strip():\n",
    "            parts.append(rest[1])\n",
    "            code_or_not.append(0)\n",
    "    # if there is not code block\n",
    "    if not parts:\n",
    "        parts.append(body)\n",
    "        code_or_not=[0]\n",
    "    # post is used to store line by line info\n",
    "    post = []\n",
    "    natural_part = []\n",
    "    code_part = []\n",
    "    for index, value in enumerate(parts):\n",
    "        if code_or_not[index]:\n",
    "#             dic, post = tokenize_code(value, dic, post)\n",
    "            code_part.append(value)\n",
    "        else:\n",
    "#             token = tokenize_natural_language(value)\n",
    "#             post.append([value, token])\n",
    "#             dic['tokens'] += token\n",
    "            natural_part.append(value)\n",
    "    # df for each post, do line by line annotation\n",
    "\n",
    "#     tokenization.append([p_id, dic['text'], dic['tokens']])\n",
    "#     count +=1\n",
    "    \n",
    "    # df for all posts\n",
    "#     results = pd.DataFrame(tokenization, columns = ['post_id', 'text', 'tokens'])\n",
    "\n",
    "    return natural_part, code_part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sentence_has_irregular_token(sentence, irregular_token):\n",
    "    for token in sentence:\n",
    "        if token in irregular_token:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_dictionary = enchant.Dict(\"en_US\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_frequency_dic = {} # [token_content] : frequency\n",
    "df = pd.DataFrame.from_csv('../processed_data/tokenizer_result/all/overall_results.csv')\n",
    "generated_tokens = [ast.literal_eval(t) for t in df.tokens.tolist()]\n",
    "\n",
    "for sentence in generated_tokens:\n",
    "    for token in sentence:\n",
    "        if token not in token_frequency_dic.keys():\n",
    "            token_frequency_dic[token] = 1\n",
    "        else:\n",
    "            token_frequency_dic[token] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "top_20 = {}\n",
    "floating_literal = r'''^\\d+\\.\\d*(?:[eE][+-]\\d+)?i?     \n",
    "                | \\d+[eE][+-]\\d+i?               \n",
    "                | \\.\\d+(?:[eE][+-]\\d+)?i?$     \n",
    "                '''\n",
    "decimal_literal = r'^\\d+$'\n",
    "operators = r'^[%/\\+\\-\\*\\,;\\$><!:\\.\\|&\\^=\\(\\)\\[\\]\\{\\}\\\"\\'\\#\\?_]+$'\n",
    "\n",
    "for token, frequency in sorted(token_frequency_dic.items(), key=operator.itemgetter(1), reverse=True):\n",
    "    if english_dictionary.check(token):\n",
    "        continue\n",
    "    if re.match(decimal_literal, token):\n",
    "        # the token is an number\n",
    "        continue\n",
    "    if re.match(floating_literal, token):\n",
    "        # the token is an float number\n",
    "        continue\n",
    "    if re.match(operators, token):\n",
    "        # the token is an number\n",
    "        continue\n",
    "        \n",
    "    if token in [\"'d\",\"'m\", \"'v\", \"'re\", \"'ve\", \"'s\", \"n't\"]:\n",
    "        # special english token\n",
    "        continue\n",
    "    top_20[token] = frequency\n",
    "    if (len(top_20.keys()) == 20):\n",
    "        break\n",
    "#     print(token)\n",
    "#     print(frequency)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write = \"token:\\tfrequency\\n\"\n",
    "for token, frequency in sorted(top_20.items(), key=operator.itemgetter(1), reverse=True):\n",
    "    write += str(token) + ':\\t' + str(frequency) + '\\n'\n",
    "    \n",
    "# print(write)\n",
    "with open('../Stats/top_20_irregular.txt', 'w') as f:\n",
    "    f.write(write)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# POS Tagging. Randomly select 10 sentences from the dataset where each sentence contains at\n",
    "# least one irregular token. Apply POS tagging on the sentences based on your own tokenization\n",
    "# results. Show and discuss the tagging results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALL_codes = pd.read_csv(\"../processed_data/code.csv\")\n",
    "ALL_codes.set_index('PostId', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sentence_stop = ['\\.', '\\?','!','\\\\n','\\.\\.\\.',';']\n",
    "code_stop = ['\\\\n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_10_sentence(df):\n",
    "    selected_10_tokens = []\n",
    "    selected_10_sentences = []\n",
    "    selected_id = []\n",
    "    id_list = list(range(len(df)))\n",
    "    np.random.shuffle(id_list)\n",
    "    for l in id_list:\n",
    "        post_text = df.iloc[l]['text']\n",
    "        nature, code = differentiate_code_nl(l,post_text, ALL_codes)\n",
    "\n",
    "        for nature_part in nature:\n",
    "            stops = []\n",
    "            for punc in sentence_stop:\n",
    "                stops += [m.start() for m in re.finditer(punc, nature_part)]\n",
    "\n",
    "            begin = 0\n",
    "            for i in sorted(stops):\n",
    "                sentence = nature_part[begin:i+1]\n",
    "                begin = i+1\n",
    "\n",
    "                sentence_token = tokenize_natural_language(sentence)\n",
    "                if sentence_has_irregular_token(sentence_token, top_20.keys()):\n",
    "                    selected_10_tokens.append(sentence_token)\n",
    "                    selected_id.append(df.iloc[l]['post_id'])\n",
    "                    selected_10_sentences.append(sentence)\n",
    "                if (len(selected_10_sentences) >= 10):\n",
    "                    return selected_10_tokens,selected_10_sentences, selected_id\n",
    "                \n",
    "        for code_part in code:\n",
    "            stops = []\n",
    "            for punc in code_stop:\n",
    "                stops += [m.start() for m in re.finditer(punc, code_part)]\n",
    "\n",
    "            begin = 0\n",
    "            for i in sorted(stops):\n",
    "                sentence = code_part[begin:i+1]\n",
    "                begin = i+1\n",
    "                sentence_token = tokenize_code(sentence)\n",
    "                if sentence_has_irregular_token(sentence_token, top_20.keys()):\n",
    "                    if sentence in selected_10_sentences:\n",
    "                        continue\n",
    "                    selected_10_tokens.append(sentence_token)\n",
    "                    selected_id.append(df.iloc[l]['post_id'])\n",
    "                    selected_10_sentences.append(sentence)\n",
    "                if (len(selected_10_sentences) >= 10):\n",
    "                    return selected_10_tokens,selected_10_sentences, selected_id\n",
    "                \n",
    "    return selected_10_tokens,selected_10_sentences, selected_id\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_tokens, res_questions, res_id = select_10_sentence(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "with open('../Stats/postagging_10_sentences_result.txt', 'w') as f:\n",
    "    f.write(\"Sentence Selected:\\n\")\n",
    "    count = 1\n",
    "    for sentence in res_questions:\n",
    "        f.write(str(count) + \". \" + sentence)\n",
    "        if sentence[-1] != '\\n':\n",
    "            f.write('\\n')\n",
    "        count += 1\n",
    "    f.write(\"\\nRunning Result:\\n\")\n",
    "    count = 1\n",
    "    for token in res_tokens:\n",
    "        f.write(str(count) + '.')\n",
    "        for i in nltk.pos_tag(token):\n",
    "            f.write(\"\\t\" + str(i) + \"\\n\")\n",
    "        count += 1\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "print(open('../Stats/postagging_10_sentences_result.txt', 'r').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
