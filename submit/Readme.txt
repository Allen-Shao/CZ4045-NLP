Dependencies:
	- Conda (https://conda.io/docs/user-guide/install/index.html)

	- Python 3.6.2 Anaconda (comes with Conda)

	- Pandas (https://pandas.pydata.org/pandas-docs/stable/install.html)

	- NLTK (http://www.nltk.org/install.html)

	- TensorFlow (https://www.tensorflow.org/install/)

Dataset:
	- selected_answers.csv (https://www.dropbox.com/s/98412ckkxnfmq2a/selected_answers.csv?dl=0)
		This file contains all the answers from the selected 700 threads.

	- selected_questions.csv (https://www.dropbox.com/s/1xt8n701nd54x74/selected_questions.csv?dl=0)
		This file contains all the questions from the selected 700 threads.

	- 100_testset_post.csv (https://www.dropbox.com/s/mg9bbtvv0y925id/100_testset_post.csv?dl=0)
		This file contains 100 posts for annotation as well as later testing.

	- annotated_results (https://www.dropbox.com/sh/dxdyt0ujh6dk4xo/AADxXgoNDfoJ--cpF6Hb03-Sa?dl=0)
		This folder contains 100 annotated posts.

	- code.csv (https://www.dropbox.com/s/ovd7fjy1zxt62u0/code.csv?dl=0)
		This file contains all the code blocks filtered from the dataset containing 700 threads.

	- input.txt (https://www.dropbox.com/s/lbgbf6jpywe6gfx/input.txt?dl=0) 
		This file contains all the question titles from the dataset.

File Hierarchy:
	- collect_data.py 
		This file is to collect and convert data from the original "Posts.xml" to several Pandas Dataframe and then store them in the csv format for further processing. The output include 3 files: "questions.csv" including all the question posts, "answers.csv" including all the answers posts, "code.csv" including code blocks in each post accordingly.

	- stemmer.py
		This file is a stemmer that takes in all the selected posts and outputs the stemming result, also include a word frequency analysis.

	- pos_tagging.py
		This file is a simple tool for pos tagging. It takes a sentence as input and outputs the pos tagging result.

	- tokenizer.py
		This file is the tokenizer we implemented for this project. The tokenizer will take code.csv, questions.csv and answers.csv (generated by collect_data.py) together with the test set used for hand annotation and testing. It will output the tokenized result. 

	- tester.py
		This file is for testing our generated tokens by comparing with the hand annotation one. It takes two directories as input, one is the generated tokens and the other is the hand annotated tokens. And it will output a report that contains all the wrong tokens.

	- Application/
		This file contains all the code for 
		- char_lstm.py
			This file is to train an LSTM model to generate question title character by character. It will generate an test sample after each training epoch. The training data should be put as ./data/input.txt or modify the --data_dir arguments to change the input data directory.

		- word_lstm_model.py
			This file contains the function to construct the model for word_lstm application.

		- word_lstm_sample.py
			This file is to generate test sample for the model generated from word_lstm_model.py. 

		- word_lstm_train.py
			This file is to train an LSTM model to generate question title word by word. The training data should be put as ./data/input.txt or modify the --data_dir arguments to change the input data directory.
			
		- utils.py
			This file provides all the utility functions for char_lstm and word_lstm applications.

Usage:
	1. Install all the dependencies through pip or conda.

	2. Download our dataset from the given link.

	3. Use Python to run the scripts. And use "-h" or "--help" to see all the input arguments and detailed usage for each file.
		$ python xxx.py -h

	    Example output:
	    % python stemmer.py -h                      
		usage: stemmer.py [-h] [--selected_questions SELECTED_QUESTIONS]
		                  [--selected_answers SELECTED_ANSWERS]
		                  [--output_file OUTPUT_FILE]

		optional arguments:
		  -h, --help            show this help message and exit
		  --selected_questions SELECTED_QUESTIONS
		                        the csv file containing all the selected questions
		  --selected_answers SELECTED_ANSWERS
		                        the csv file containing all the selected answers
		  --output_file OUTPUT_FILE
		                        the output file