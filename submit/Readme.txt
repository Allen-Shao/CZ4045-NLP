Dependencies:
	- Conda (https://conda.io/docs/user-guide/install/index.html)
	- Python 3.6.2 Anaconda (comes with Conda)
	- Pandas (https://pandas.pydata.org/pandas-docs/stable/install.html)
	- NLTK (http://www.nltk.org/install.html)
	- TensorFlow (https://www.tensorflow.org/install/)

Dataset:
	- selected_answers.csv (https://www.dropbox.com/s/gbhibawnwpm19h0/selected_answers.csv?dl=0)
		This file contains all the answers from the selected 700 threads.
	- selected_questions.csv (https://www.dropbox.com/s/7witiicpk4fedch/selected_questions.csv?dl=0)
		This file contains all the questions from the selected 700 threads.
	- 100_testset_post.csv (https://www.dropbox.com/s/jegpok2yia56tau/100_testset_post.csv?dl=0)
		This file contains 100 posts for annotation as well as later testing.
	- annotated_results.csv (https://www.dropbox.com/sh/ga0gmlj1kbx6yt8/AACEE9KuQ0xYNET11-prDAeta?dl=0)
		This folder contains 100 annotated posts.
	- code.csv (#TO BE ADDED)
		This file contains all the code blocks filtered from the dataset containing 700 threads.

File Hierarchy:
	- collect_data.py 
		This file is to collect and convert data from the original "Posts.xml" to several Pandas Dataframe and then store them in the csv format for further processing. The output include 3 files: "questions.csv" including all the question posts, "answers.csv" including all the answers posts, "code.csv" including code blocks in each post accordingly.
	- stemmer.py
		This file is a stemmer that takes in all the selected posts and outputs the stemming result, also include a word frequency analysis.
	- pos_tagging.py
		This file is a simple tool for pos tagging. It takes a sentence as input and outputs the pos tagging result
	- tokenizer.py
		This file is the tokenizer we implemented for this project. The tokenizer will take code.csv, questions.csv and answers.csv (generated by collect_data.py) together with the test set used for hand annotation and testing. It will output the tokenized result. 
	- tester.py
		This file is for testing our generated tokens by comparing with the hand annotation one. It takes two directories as input, one is the generated tokens and the other is the hand annotated tokens. And it will output a report that contains all the wrong tokens.

Usage:
	1. Install all the dependencies through pip or conda.
	2. Use Python to run the scripts. And use "-h" or "--help" to see all the input arguments and detailed usage for each file.
			$ python xxx.py -h

	   Example output:
	    % python stemmer.py -h                      
		usage: stemmer.py [-h] [--selected_questions SELECTED_QUESTIONS]
		                  [--selected_answers SELECTED_ANSWERS]
		                  [--output_file OUTPUT_FILE]

		optional arguments:
		  -h, --help            show this help message and exit
		  --selected_questions SELECTED_QUESTIONS
		                        the csv file containing all the selected questions
		  --selected_answers SELECTED_ANSWERS
		                        the csv file containing all the selected answers
		  --output_file OUTPUT_FILE
		                        the output file